{
  "job_description": "We are seeking a highly skilled and motivated AI/ML Engineer to design, develop, and deploy state-of-the-art machine learning models and AI-driven systems.\n\nThe ideal candidate will have a strong background in machine learning frameworks, programming, and data analysis, coupled with the ability to work in a collaborative, fast-paced environment.\n\nKey Responsibilities\n\n Design, implement, and optimize machine learning models for various applications, including prediction, classification, clustering, and recommendation systems.\n Develop and maintain data pipelines, ensuring data is clean, reliable, and suitable for training and testing machine learning models.\n Stay updated on the latest AI/ML trends, techniques, and tools to drive innovation within the team.\n Prototype and test new algorithms and methods to solve complex problems.\n Collaborate with software engineers to integrate AI/ML solutions into existing platforms and applications.\n Monitor and analyze the performance of deployed models, implementing improvements as needed to maintain high accuracy and efficiency.\n Work closely with cross-functional teams, including data scientists, product managers, and business analysts, to align AI/ML solutions with business objectives.",
  "resume": "ABHIRAM YERRAMSETTI\nyerramsetti.abhiram@gmail.com\ngithub.com/abhiram-yerramsetti+91 7569566423\nlinkedin.com/in/yerramsetti-abhiram\nProfessional Experience\nData Science Intern | Innomatics Research Labs (GenAI Intern) 01/2025 – 03/2025\n•Built real-time NLP pipelines using Python and Google GenAI for intelligent code correction.\n•Used LangChain to chain prompts, manage memory, and create dynamic conversation flows.\n•Integrated backend automation scripts to deploy and scale GenAI applications.\n•Collaborated with cross-functional teams to refine system configuration and optimize debugging tools\nIndependent NLP Research & Projects\n2022 – 2024 (Self-Directed Learning & Application)\nDuring this period, I focused on strengthening my NLP, machine learning, and data science skills by building real-world projects, \ncontributing to open-source tools, and completing specialized training.\nPersonalized AI Code Corrector & Explainer\nTechnologies: Google GenAI, NLP, Prompt Engineering, Python, Text Generation\n•Built a multi-step debugging assistant using LangChain, capable of detecting, explaining, and fixing code issues.\n•Employed prompt chaining, memory management, and function calling to enable contextual corrections.\n•Automated input preprocessing, classification, and result interpretation for seamless integration.\n•Supported real-time input and batch mode for scalability.\nReal-Time Chat Data Cleaning & Text Analysis\nTechnologies: Python, NLTK, EDA, Text Preprocessing, NLP\n•Processed and cleaned over 40,000 chat messages using NLTK and custom scripts for noise reduction and pattern extraction.\n•Performed exploratory data analysis and prepared data for sentiment and topic modeling. \nFIA Lap Time Prediction System\nTechnologies: Regression, KNN, ANN, Feature Engineering, Python\n•Analyzed 300K+ rows of racing data and implemented multiple models to predict lap times.\n•Achieved 92% accuracy using ANN, demonstrating effective preprocessing and hyperparameter tuning. \nLicense Plate Recognition\nYOLOv8, EasyOCR, OpenCV\n•Created a computer vision pipeline to detect and read license plates from live footage.\n•Integrated a user-friendly dashboard using Streamlit and real-time detection alerts.\nAI Door Alarm System\nCNN, TensorFlow\n•Built a surveillance system that uses CNN models to detect human presence and send live alerts.\n•Handled edge-case detection using OpenCV enhancements and real-time feedback loops.\nSkills\nProgramming:\nPython (proficient), SQL\nDevelopment Concepts:\n Basic knowledge of SDLC phases and Agile principles\nData Tools:\nOpenCV, Pandas, NumPy, EasyOCR, Streamlit, MySQL\nLangChain & LLMs:\nPrompt chaining, memory, OpenAI API integration, \nconversational agentsSoft Skills:\nCollaboration, Problem Solving, Documentation\nSoftware Configuration:\nPython scripting, basic environment setup, automation for \npipelines\nMachine Learning:\nScikit-learn, TensorFlow, Keras, NLTK\nEducation\nBachelor of Engineering in Computer Science\nOsmania University2018 – 2022\nCertificates\nAdvanced Data Science with Python\n – NASSCOMExploratory Data Analysis (EDA)\n – Innomatics Research LabsData Science Course Completion\n – Innomatics Research Labs (2024) ",
  "questions": [
    "1.  Your résumé mentions building real-time NLP pipelines using Python and Google GenAI. Can you describe a specific challenge you faced during this project and how you overcame it?",
    "2.  The job description requires optimizing machine learning models. Can you discuss a time when you significantly improved the performance of a model you developed, and what techniques you used?",
    "3.  You mentioned using LangChain for prompt chaining and memory management. How would you apply these techniques to develop a recommendation system?",
    "4.  This role involves collaborating with cross-functional teams. Can you describe a time when you had to explain a complex AI/ML concept to a non-technical stakeholder?",
    "5.  The job description emphasizes staying updated on the latest AI/ML trends. How do you stay current with the rapidly evolving landscape of machine learning, and can you share a recent AI/ML trend that excites you?"
  ],
  "answers": [
    "Challenge: While building real-time NLP pipelines with Python and Google GenAI, I faced latency issues when processing high-volume streaming data for entity extraction and graph updates.\n\nSolution: I optimized the pipeline by adding async processing (asyncio), micro-batching data before API calls, and using a cache (Redis) to reduce redundant GenAI requests. I also switched to networkx for efficient graph handling.\n\nResult: This reduced latency by ~40% and made the pipeline handle larger data streams smoothly.",
    "What I did:\n I cleaned and balanced the dataset to reduce bias.\n I tuned hyperparameters using grid search and cross-validation.\n I switched from a basic TF-IDF + Logistic Regression to a fine-tuned BERT model for better context understanding.\n I applied techniques like early stopping and learning rate scheduling to avoid overfitting.\n\nResult: These changes improved the model’s F1-score by over 20%, and it generalized much better on unseen data.",
    "I’d apply LangChain in a recommendation system by chaining prompts that:\n\nFirst gather user preferences through conversation,\n\nThen query product data or a knowledge base,\n\nAnd finally generate recommendations that consider both recent inputs and past interactions using LangChain’s memory (like ConversationBufferMemory).\n\nThis approach ensures recommendations feel personalized and adapt over time without needing complex retraining between sessions.\n\n",
    "In my internship, I had to explain how our NLP model’s entity extraction worked to a product manager. I broke it down using simple terms — comparing entities to “highlighting important names and terms in text, like a human would when reading.”\n\nI also shared examples showing how the model identified key relationships, avoiding technical jargon like embeddings or tokenization. This helped them understand the model’s value for the product without getting lost in details.",
    "I stay updated by following AI/ML blogs (like Google AI and OpenAI), reading papers on arXiv, and joining communities on GitHub and LinkedIn.\n\nA recent trend that excites me is the rise of retrieval-augmented generation (RAG) — combining LLMs with external knowledge sources to improve accuracy and reduce hallucinations. I see huge potential for building more reliable and dynamic AI systems with this approach."
  ],
  "score_output": "Here are the scores for each candidate answer:\n\nQ1: 9\nQ2: 8\nQ3: 7\nQ4: 7\nQ5: 8\n"
}