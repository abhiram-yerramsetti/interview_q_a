{
  "job_description": "Design, implement, and optimize machine learning models for various applications, including prediction, classification, clustering, and recommendation systems.\n Develop and maintain data pipelines, ensuring data is clean, reliable, and suitable for training and testing machine learning models.\n Stay updated on the latest AI/ML trends, techniques, and tools to drive innovation within the team.\n Prototype and test new algorithms and methods to solve complex problems.\n Collaborate with software engineers to integrate AI/ML solutions into existing platforms and applications.\n Monitor and analyze the performance of deployed models, implementing improvements as needed to maintain high accuracy and efficiency.\n Work closely with cross-functional teams, including data scientists, product managers, and business analysts, to align AI/ML solutions with business objective",
  "resume": "ABHIRAM YERRAMSETTI\nyerramsetti.abhiram@gmail.com\ngithub.com/abhiram-yerramsetti+91 7569566423\nlinkedin.com/in/yerramsetti-abhiram\nProfessional Experience\nData Science Intern | Innomatics Research Labs (GenAI Intern) 01/2025 – 03/2025\n•Built real-time NLP pipelines using Python and Google GenAI for intelligent code correction.\n•Used LangChain to chain prompts, manage memory, and create dynamic conversation flows.\n•Integrated backend automation scripts to deploy and scale GenAI applications.\n•Collaborated with cross-functional teams to refine system configuration and optimize debugging tools\nIndependent NLP Research & Projects\n2022 – 2024 (Self-Directed Learning & Application)\nDuring this period, I focused on strengthening my NLP, machine learning, and data science skills by building real-world projects, \ncontributing to open-source tools, and completing specialized training.\nPersonalized AI Code Corrector & Explainer\nTechnologies: Google GenAI, NLP, Prompt Engineering, Python, Text Generation\n•Built a multi-step debugging assistant using LangChain, capable of detecting, explaining, and fixing code issues.\n•Employed prompt chaining, memory management, and function calling to enable contextual corrections.\n•Automated input preprocessing, classification, and result interpretation for seamless integration.\n•Supported real-time input and batch mode for scalability.\nReal-Time Chat Data Cleaning & Text Analysis\nTechnologies: Python, NLTK, EDA, Text Preprocessing, NLP\n•Processed and cleaned over 40,000 chat messages using NLTK and custom scripts for noise reduction and pattern extraction.\n•Performed exploratory data analysis and prepared data for sentiment and topic modeling. \nFIA Lap Time Prediction System\nTechnologies: Regression, KNN, ANN, Feature Engineering, Python\n•Analyzed 300K+ rows of racing data and implemented multiple models to predict lap times.\n•Achieved 92% accuracy using ANN, demonstrating effective preprocessing and hyperparameter tuning. \nLicense Plate Recognition\nYOLOv8, EasyOCR, OpenCV\n•Created a computer vision pipeline to detect and read license plates from live footage.\n•Integrated a user-friendly dashboard using Streamlit and real-time detection alerts.\nAI Door Alarm System\nCNN, TensorFlow\n•Built a surveillance system that uses CNN models to detect human presence and send live alerts.\n•Handled edge-case detection using OpenCV enhancements and real-time feedback loops.\nSkills\nProgramming:\nPython (proficient), SQL\nDevelopment Concepts:\n Basic knowledge of SDLC phases and Agile principles\nData Tools:\nOpenCV, Pandas, NumPy, EasyOCR, Streamlit, MySQL\nLangChain & LLMs:\nPrompt chaining, memory, OpenAI API integration, \nconversational agentsSoft Skills:\nCollaboration, Problem Solving, Documentation\nSoftware Configuration:\nPython scripting, basic environment setup, automation for \npipelines\nMachine Learning:\nScikit-learn, TensorFlow, Keras, NLTK\nEducation\nBachelor of Engineering in Computer Science\nOsmania University2018 – 2022\nCertificates\nAdvanced Data Science with Python\n – NASSCOMExploratory Data Analysis (EDA)\n – Innomatics Research LabsData Science Course Completion\n – Innomatics Research Labs (2024) ",
  "questions": [
    "1.  The job description mentions optimizing machine learning models. Can you describe a time when you significantly improved the performance (e.g., accuracy, efficiency) of an existing model, and what techniques did you use?",
    "2.  You mentioned building real-time NLP pipelines using Python and Google GenAI. Can you elaborate on the challenges you faced integrating the backend automation scripts for deployment and scaling, and how you overcame them?",
    "3.  The job requires developing and maintaining data pipelines. Describe your experience with building data pipelines for machine learning, including the tools you used and the steps you took to ensure data quality and reliability.",
    "4.  This role involves collaborating with cross-functional teams. Can you share an example from your experience where you had to explain a complex AI/ML concept or result to a non-technical audience, and how you ensured they understood it?",
    "5.  The job description mentions staying updated on the latest AI/ML trends. Besides your project experience, how do you actively stay current with new advancements and techniques in the field of machine learning?"
  ],
  "answers": [
    "I faced challenges scaling Python scripts as data volume grew — hitting API rate limits and latency issues. I overcame this by containerizing with Docker, using Cloud Functions for auto-scaling, adding async processing, and micro-batching API calls. This made the system responsive and cost-efficient.",
    "I built data pipelines using Python (pandas, PySpark), Google Cloud Storage, and Cloud Functions. I ensured data quality by adding validation checks, logging anomalies, tracking dataset versions, and automating runs with monitoring to catch issues early. This kept the data clean and reliable for ML models",
    "I once explained entity extraction to a product manager by saying: “The model highlights key names and terms, like underlining important parts when we read.” I used real examples, avoided jargon, and encouraged questions to ensure clarity.",
    "I once explained entity extraction to a product manager by saying: “The model highlights key names and terms, like underlining important parts when we read.” I used real examples, avoided jargon, and encouraged questions to ensure clarity.",
    "I follow arXiv, Google AI, OpenAI, and Hugging Face blogs, and stay active on GitHub and LinkedIn AI communities. A trend I’m excited about is retrieval-augmented generation (RAG), which improves model accuracy by combining LLMs with external data sources."
  ],
  "score_output": "Q1: 6\nQ2: 7\nQ3: 4\nQ4: 8\nQ5: 9\n"
}